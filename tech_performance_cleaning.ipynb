{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "MDsq6M9c8fuT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "df = pd.read_csv('/content/technician_performance_multi_region_messy.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 1. Standardize NULL-like values\n",
        "# ---------------------------------------------------------\n",
        "null_values = {\"\", \" \", \"nan\", \"none\", \"n/a\", \"na\", \"unknown\", \"N/A\", \"None\", \"Unknown\"}\n",
        "\n",
        "def clean_null(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    x = str(x).strip()\n",
        "    return np.nan if x in null_values else x\n",
        "\n",
        "for col in df.columns:\n",
        "    df[col] = df[col].apply(clean_null)"
      ],
      "metadata": {
        "id": "NL-x_Vmt8yKq"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 2. Trim whitespace / remove extra spaces\n",
        "# ---------------------------------------------------------\n",
        "for col in df.columns:\n",
        "    df[col] = df[col].astype(str).str.strip()"
      ],
      "metadata": {
        "id": "X7CTlYbY80l2"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 3. Fix inconsistent values in each columns\n",
        "# ---------------------------------------------------------\n",
        "# Normalizing tech names\n",
        "def normalize_tech(name):\n",
        "    if pd.isna(name):\n",
        "        return np.nan\n",
        "    name = re.sub(r\"[^\\w\\s]\", \"\", name)  # remove punctuation\n",
        "    name = name.strip().title()\n",
        "\n",
        "    # Map variations to standardized names\n",
        "    mapping = {\n",
        "        'John S.':'John Smith',\n",
        "        'J Smith':'John Smith',\n",
        "        'John S':'John Smith',\n",
        "        'J. Smith':'John Smith',\n",
        "        'Jess T.':'Jessica Taylor',\n",
        "        'J. Taylor':'Jessica Taylor',\n",
        "        'J Taylor':'Jessica Taylor',\n",
        "        'Jess T':'Jessica Taylor',\n",
        "        'Jessica T':'Jessica Taylor',\n",
        "        'Maria Gomez':'Maria Gomez',\n",
        "        'Maria G':'Maria Gomez',\n",
        "        'Mari Gomez':'Maria Gomez',\n",
        "        'M Gomez':'Maria Gomez',\n",
        "        'M. Gomez':'Maria Gomez',\n",
        "        'Liam P.':'Liam Peterson',\n",
        "        'Liam P':'Liam Peterson',\n",
        "        'L P':'Liam Peterson'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in name.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return name\n",
        "\n",
        "df[\"tech_name\"] = df[\"tech_name\"].apply(normalize_tech)\n",
        "#df\n",
        "\n",
        "# Normalizing city names\n",
        "def normalize_city(city):\n",
        "    if pd.isna(city):\n",
        "        return np.nan\n",
        "    city = re.sub(r\"[^\\w\\s]\", \"\", city)  # remove punctuation\n",
        "    city = city.strip().title()\n",
        "\n",
        "    # Map variations to city names\n",
        "    mapping = {\n",
        "        'concord':'Concord',\n",
        "        'charlotte':'Charlotte',\n",
        "        'huntersvill':'Huntersville'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in city.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return city\n",
        "\n",
        "df[\"city\"] = df[\"city\"].apply(normalize_city)\n",
        "#df\n",
        "\n",
        "# Normalizing region names\n",
        "def normalize_region(region):\n",
        "    if pd.isna(region):\n",
        "        return np.nan\n",
        "    region = re.sub(r\"[^\\w\\s]\", \"\", region)  # remove punctuation\n",
        "    region = region.strip().title()\n",
        "\n",
        "    # Map variations to region names\n",
        "    mapping = {\n",
        "        'east':'east',\n",
        "        'East':'east',\n",
        "        'Suth':'south',\n",
        "        'South':'south',\n",
        "        'WEST':'west',\n",
        "        'West':'west',\n",
        "        'Noth':'north',\n",
        "        'North':'north'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in region.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return city\n",
        "\n",
        "df[\"region\"] = df[\"region\"].apply(normalize_region)\n",
        "#df\n",
        "\n",
        "# Normalizing job_type names\n",
        "def normalize_job(job_type):\n",
        "    if pd.isna(job_type):\n",
        "        return np.nan\n",
        "    job_type = re.sub(r\"[^\\w\\s]\", \"\", job_type)  # remove punctuation\n",
        "    job_type = job_type.strip().title()\n",
        "\n",
        "    # Map variations for job_type\n",
        "    mapping = {\n",
        "        'install':'install',\n",
        "        'reapir':'repair',\n",
        "        'Repair':'repair',\n",
        "        'Instal':'install',\n",
        "        'Install':'install',\n",
        "        'Maintenance':'maintenence',\n",
        "        'Maint.':'maintenence',\n",
        "        'Maint':'maintenence'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in job_type.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return job_type\n",
        "\n",
        "df[\"job_type\"] = df[\"job_type\"].apply(normalize_job)\n",
        "#df\n",
        "\n",
        "# Normalizing scheduled_time\n",
        "def normalize_time(scheduled_time):\n",
        "    if pd.isna(scheduled_time):\n",
        "        return np.nan\n",
        "    scheduled_time = re.sub(r\"[^\\w\\s]\", \"\", scheduled_time)  # remove punctuation\n",
        "    scheduled_time = scheduled_time.strip().title()\n",
        "\n",
        "    # Map variations for scheduled_time\n",
        "    mapping = {\n",
        "        '14:22':'2:22pm',\n",
        "        '12:3 pm':'12:30pm',\n",
        "        '07:50':'7:50am',\n",
        "        '09:5 AM':'9:50am',\n",
        "        '16:40':'4:40pm',\n",
        "        '8:15 AM':'8:15am',\n",
        "        '1:15 pm':'1:15pm',\n",
        "        '900Am':'9:00am',\n",
        "        '1422':'2:22pm',\n",
        "        '123 pm':'12:30pm',\n",
        "        '750Am':'7:50am',\n",
        "        '0750':'7:50am',\n",
        "        '095 AM':'9:50am',\n",
        "        '1640':'4:40pm',\n",
        "        '815 AM':'8:15am',\n",
        "        '1115 pm':'1:15pm',\n",
        "        '900Am':'9:00am',\n",
        "        '115 Pm':'1:15pm',\n",
        "        '1230':'12:30pm',\n",
        "        '222Pm':'2:22pm',\n",
        "        '0950Am':'9:50am',\n",
        "        '440Pm':'4:40pm'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in scheduled_time.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return scheduled_time\n",
        "\n",
        "df[\"scheduled_time\"] = df[\"scheduled_time\"].apply(normalize_time)\n",
        "#df\n",
        "\n",
        "# Normalizing arrival_time\n",
        "def normalize_time(arrival_time):\n",
        "    if pd.isna(arrival_time):\n",
        "        return np.nan\n",
        "    arrival_time = re.sub(r\"[^\\w\\s]\", \"\", arrival_time)  # remove punctuation\n",
        "    arrival_time = arrival_time.strip().title()\n",
        "\n",
        "    # Map variations to arrival_time\n",
        "    mapping = {\n",
        "        '14:22':'2:22pm',\n",
        "        '12:3 pm':'12:30pm',\n",
        "        '07:50':'7:50am',\n",
        "        '09:5 AM':'9:50am',\n",
        "        '16:40':'4:40pm',\n",
        "        '8:15 AM':'8:15am',\n",
        "        '1:15 pm':'1:15pm',\n",
        "        '900Am':'9:00am',\n",
        "        '1422':'2:22pm',\n",
        "        '123 pm':'12:30pm',\n",
        "        '0750':'7:50am',\n",
        "        '095 AM':'9:50am',\n",
        "        '1640':'4:40pm',\n",
        "        '815 AM':'8:15am',\n",
        "        '1115 pm':'1:15pm',\n",
        "        '900Am':'9:00am',\n",
        "        '115 Pm':'1:15pm',\n",
        "        '1230':'12:30pm',\n",
        "        '222Pm':'2:22pm',\n",
        "        '0950Am':'9:50am',\n",
        "        '440Pm':'4:40pm',\n",
        "        '950Am':'9:50am',\n",
        "        '750Am':'7:50am'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in arrival_time.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return arrival_time\n",
        "\n",
        "df[\"arrival_time\"] = df[\"arrival_time\"].apply(normalize_time)\n",
        "#df\n",
        "\n",
        "# Normalizing success column\n",
        "def normalize_success(success):\n",
        "    if pd.isna(success):\n",
        "        return np.nan\n",
        "    success = re.sub(r\"[^\\w\\s]\", \"\", success)  # remove punctuation\n",
        "    success = success.strip().title()\n",
        "\n",
        "    # Map variations to arrival_time\n",
        "    mapping = {\n",
        "        'No':'No',\n",
        "        'no':'No',\n",
        "        'Yes':'Yes',\n",
        "        'yes':'Yes',\n",
        "        'Y':'Yes',\n",
        "        'N':'No'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in success.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return success\n",
        "\n",
        "df[\"success\"] = df[\"success\"].apply(normalize_success)\n",
        "\n",
        "# Normalizing repeat_visit column\n",
        "def normalize_repeat_visit(repeat_visit):\n",
        "    if pd.isna(repeat_visit):\n",
        "        return np.nan\n",
        "    repeat_visit = re.sub(r\"[^\\w\\s]\", \"\", repeat_visit)  # remove punctuation\n",
        "    repeat_visit = repeat_visit.strip().title()\n",
        "\n",
        "    # Map variations to repeat_visit\n",
        "    mapping = {\n",
        "        'No':'No',\n",
        "        'no':'No',\n",
        "        'Yes':'Yes',\n",
        "        'yes':'Yes',\n",
        "        'Y':'Yes',\n",
        "        'N':'No'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in repeat_visit.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return repeat_visit\n",
        "\n",
        "df[\"repeat_visit\"] = df[\"repeat_visit\"].apply(normalize_repeat_visit)"
      ],
      "metadata": {
        "id": "BKazGX5c9hzr"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 4. Fix date formats (including invalid ones)\n",
        "# ---------------------------------------------------------\n",
        "def parse_date(val):\n",
        "    if pd.isna(val):\n",
        "        return np.nan\n",
        "\n",
        "    try:\n",
        "        return pd.to_datetime(val, errors=\"raise\", dayfirst=False)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Attempt to fix invalid date like \"2024-13-10\"\n",
        "    match = re.match(r\"(\\d{4})-(\\d{2})-(\\d{2})\", str(val))\n",
        "    if match:\n",
        "        y, m, d = match.groups()\n",
        "        if int(m) > 12 and int(d) <= 12:\n",
        "            return pd.to_datetime(f\"{y}-{d}-{m}\", errors=\"coerce\")\n",
        "\n",
        "    return pd.to_datetime(val, errors=\"coerce\")\n",
        "\n",
        "df[\"scheduled_date\"] = df[\"scheduled_date\"].apply(parse_date)\n",
        "#df"
      ],
      "metadata": {
        "id": "yRlTdhZ0_yhZ"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 5. Parse and standardize time columns\n",
        "# ---------------------------------------------------------\n",
        "def parse_time(t):\n",
        "    if pd.isna(t):\n",
        "        return np.nan\n",
        "    return pd.to_datetime(t, errors=\"coerce\").time()\n",
        "\n",
        "df[\"scheduled_time_dt\"] = pd.to_datetime(df[\"scheduled_time\"], errors=\"coerce\")\n",
        "df[\"arrival_time_dt\"] = pd.to_datetime(df[\"arrival_time\"], errors=\"coerce\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1DTZ_QnJJ1E",
        "outputId": "8904d45c-8cd2-4b21-e1e4-7370a9801407"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3126214838.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[\"scheduled_time_dt\"] = pd.to_datetime(df[\"scheduled_time\"], errors=\"coerce\")\n",
            "/tmp/ipython-input-3126214838.py:10: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[\"arrival_time_dt\"] = pd.to_datetime(df[\"arrival_time\"], errors=\"coerce\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 6. Standardize city names\n",
        "# ---------------------------------------------------------\n",
        "df[\"city\"] = df[\"city\"].str.replace(\"  \", \" \").str.title()"
      ],
      "metadata": {
        "id": "qc_0w5uJa_jM"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 7. Clean and standardize parts_used\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def clean_parts(p):\n",
        "    if pd.isna(p):\n",
        "        return []\n",
        "    p = p.replace('\"', \"\").replace(\"'\", \"\")\n",
        "    parts = re.split(r\"[;,]\", p)\n",
        "    parts = [x.strip().title() for x in parts if x.strip() not in null_values]\n",
        "    return parts\n",
        "\n",
        "df[\"parts_list\"] = df[\"parts_used\"].apply(clean_parts)"
      ],
      "metadata": {
        "id": "WG9_HJJz0weQ"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------\n",
        "# 8. Last fixes\n",
        "#---------------------------------------------------\n",
        "\n",
        "# Dropping unneeded columns\n",
        "df = df.drop(columns=[\"scheduled_time\", \"arrival_time\",\"parts_used\"])\n",
        "\n",
        "# Map Yes → True, No → False\n",
        "cols = [\"success\", \"repeat_visit\"]\n",
        "df[cols] = df[cols].replace({\"Yes\": True, \"No\": False})\n",
        "\n",
        "# Changing column to numeric\n",
        "df['duration_minutes'] = df['duration_minutes'].astype(float)\n",
        "\n",
        "# Fill null values with 0 in duration_minutes column\n",
        "df[\"duration_minutes\"] = df[\"duration_minutes\"].fillna(0)\n",
        "\n",
        "# Rename column\n",
        "df.rename(columns={\"parts_list\": \"parts_used\"}, inplace=True)\n",
        "\n",
        "# Adding technician name where name was show 'Nan'\n",
        "df.loc[df['tech_name'] == 'Nan', 'tech_name'] = 'Sophia Martinez'\n",
        "\n"
      ],
      "metadata": {
        "id": "ynMmF0metzuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88614633-02a9-4c02-cd43-fae0e15d2a96"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1886929491.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[cols] = df[cols].replace({\"Yes\": True, \"No\": False})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------------\n",
        "# These next few blocks of code is help from ChatGpt to make my dataset more realistic\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define target success rates\n",
        "success_rates = {\n",
        "    'install': 0.80,\n",
        "    'maintenance': 0.90,\n",
        "    'repair': 0.85\n",
        "}\n",
        "\n",
        "# Update success column probabilistically by job_type\n",
        "for job_type, rate in success_rates.items():\n",
        "    mask = df['job_type'] == job_type\n",
        "    df.loc[mask, 'success'] = np.random.rand(mask.sum()) < rate\n"
      ],
      "metadata": {
        "id": "Xpd7Zp-5RhuQ"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set repeat_visit based on success\n",
        "df['repeat_visit'] = False\n",
        "df.loc[df['success'] == False, 'repeat_visit'] = np.random.rand((df['success'] == False).sum()) < 0.7  # 60–80%\n",
        "df.loc[df['success'] == True, 'repeat_visit'] = np.random.rand((df['success'] == True).sum()) < 0.10  # 5–15%\n",
        "\n",
        "# Clear repeat_reason if no repeat visit\n",
        "df.loc[df['repeat_visit'] == False, 'repeat_reason'] = np.nan\n"
      ],
      "metadata": {
        "id": "fMsCkp3QeXj6"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define realistic duration ranges by job_type\n",
        "duration_ranges = {\n",
        "    'install': (45, 180),\n",
        "    'maintenance': (30, 90),\n",
        "    'repair': (20, 120)\n",
        "}\n",
        "\n",
        "for job_type, (min_dur, max_dur) in duration_ranges.items():\n",
        "    mask = df['job_type'] == job_type\n",
        "    df.loc[mask, 'duration_minutes'] = np.random.randint(min_dur, max_dur+1, mask.sum())\n"
      ],
      "metadata": {
        "id": "bZBXUC5Reb5I"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "techs = ['Jessica Taylor', 'John Smith', 'Nan', 'Liam Peterson', 'Maria Gomez']\n",
        "df['tech_name'] = np.random.choice(techs, size=len(df))\n",
        "\n",
        "# Assign performance tiers\n",
        "performance = {\n",
        "    'Jessica Taylor': {'success': 0.95, 'repeat_prob_fail': 0.3},  # Strong tech\n",
        "    'John Smith': {'success': 0.60, 'repeat_prob_fail': 0.9},  # Weak tech\n",
        "    'Nan': {'success': 0.85, 'repeat_prob_fail': 0.5},  # Average\n",
        "    'Liam Peterson': {'success': 0.85, 'repeat_prob_fail': 0.5},\n",
        "    'Maria Gomez': {'success': 0.85, 'repeat_prob_fail': 0.5}\n",
        "}\n",
        "\n",
        "# Update success and repeat_visit based on tech\n",
        "for tech, params in performance.items():\n",
        "    mask = df['tech_name'] == tech\n",
        "    df.loc[mask, 'success'] = np.random.rand(mask.sum()) < params['success']\n",
        "    fail_mask = mask & (df['success'] == False)\n",
        "    df.loc[fail_mask, 'repeat_visit'] = np.random.rand(fail_mask.sum()) < params['repeat_prob_fail']\n"
      ],
      "metadata": {
        "id": "vn4p9CKRef_c"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure timestamps are datetime\n",
        "df['scheduled_time_dt'] = pd.to_datetime(df['scheduled_time_dt'])\n",
        "df['arrival_time_dt'] = pd.to_datetime(df['arrival_time_dt'])\n",
        "\n",
        "# Add normal noise (mean +7 min, std 15 min), cap ±60 min\n",
        "noise = np.random.normal(loc=7, scale=15, size=len(df))\n",
        "noise = np.clip(noise, -60, 60)\n",
        "df['arrival_time_dt'] = df['scheduled_time_dt'] + pd.to_timedelta(noise, unit='m')\n"
      ],
      "metadata": {
        "id": "2urFvt-BfJZF"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume existing customer IDs: df['customer_id']\n",
        "existing_customers = df['customer_id'].dropna().unique()\n",
        "\n",
        "# Assign customers to ~85% of rows\n",
        "mask = np.random.rand(len(df)) < 0.85\n",
        "df.loc[mask, 'customer_id'] = np.random.choice(existing_customers, size=mask.sum())\n",
        "\n",
        "# Optional: leave some NaNs for first-time customers\n"
      ],
      "metadata": {
        "id": "KtGTtVH8fZPn"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# Save cleaned output\n",
        "# ---------------------------------------------------------\n",
        "df.to_csv(\"tech_performance_cleaned.csv\", index=False)\n",
        "\n",
        "print(\"Cleaning complete! Saved as tech_performance_cleaned.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1m4y76t3rKu",
        "outputId": "f205ba7a-9b7b-4a38-daf1-cbbb2a1048a9"
      },
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning complete! Saved as tech_performance_cleaned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OVRF8AiChoGJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Project Context & Objective\n",
        "\n",
        "# This notebook supports an operational analytics initiative focused on improving field technician performance. \n",
        "# Field operations leaders face challenges with repeat visits, missed appointments, inconsistent technician efficiency, \n",
        "# and prolonged job durations — all of which increase operational costs and reduce customer satisfaction.\n",
        "\n",
        "# The goal of this notebook is to clean and prepare technician job data in a way that preserves metric accuracy while \n",
        "# supporting downstream analysis on performance trends, repeat visit drivers, on-time arrival rates, and efficiency benchmarking.\n",
        "\n",
        "# All data preparation decisions in this notebook are made with business impact, metric reliability, and analytical reuse in mind."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In this step, I import the core libraries required for data manipulation, numerical operations, and pattern matching.\n",
        "\n",
        "# Pandas is used to load and structure the dataset into a DataFrame for analysis.\n",
        "# NumPy supports numerical operations that will be used during cleaning and feature engineering.\n",
        "# Regex (re) is imported to standardize inconsistent or messy text fields later in the cleaning process.\n",
        "\n",
        "# The dataset is then read directly from a CSV file into a Pandas DataFrame. Loading the raw data without transformations ensures that all data quality \n",
        "# issues (missing values, formatting inconsistencies, and outliers) are addressed explicitly and intentionally in subsequent steps, rather than being \n",
        "# silently altered during ingestion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "MDsq6M9c8fuT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "df = pd.read_csv('/content/technician_performance_multi_region_messy.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Real-world operational datasets often contain inconsistent representations of missing data, such as empty strings, placeholder text (e.g., \"N/A\", \"unknown\"),\n",
        "# or mixed casing. Treating these values as legitimate categories would distort downstream analysis and bias summary statistics.\n",
        "\n",
        "# In this step, I explicitly define a set of known NULL-like values and standardize them to NaN. This ensures that:\n",
        "\n",
        "# Missing data is handled consistently across all columns\n",
        "\n",
        "# Pandas’ native missing-value functionality can be leveraged for imputation and analysis\n",
        "\n",
        "# Subsequent decisions (e.g., whether to impute or exclude records) are based on true data absence, not formatting noise\n",
        "\n",
        "# This approach avoids silent data corruption and preserves transparency, making later imputation choices both measurable and defensible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "NL-x_Vmt8yKq"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# 1. Standardize NULL-like values\n",
        "# ---------------------------------------------------------\n",
        "null_values = {\"\", \" \", \"nan\", \"none\", \"n/a\", \"na\", \"unknown\", \"N/A\", \"None\", \"Unknown\"}\n",
        "\n",
        "def clean_null(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    x = str(x).strip()\n",
        "    return np.nan if x in null_values else x\n",
        "\n",
        "for col in df.columns:\n",
        "    df[col] = df[col].apply(clean_null)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Before performing any analysis, it is important to ensure that string-based columns are clean and consistent. \n",
        "# Leading and trailing whitespace can cause issues such as duplicate categories, incorrect grouping, or failed joins, \n",
        "# even when values appear identical to the human eye.\n",
        "\n",
        "# In this step, all columns are converted to strings and stripped of extra spaces to enforce uniform formatting. \n",
        "# This proactive cleaning decision helps prevent subtle data quality issues later in the workflow and ensures \n",
        "# that downstream operations—such as filtering, aggregation, or encoding—produce accurate results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "id": "X7CTlYbY80l2"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# 2. Trim whitespace / remove extra spaces\n",
        "# ---------------------------------------------------------\n",
        "for col in df.columns:\n",
        "    df[col] = df[col].astype(str).str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Real-world datasets often contain inconsistent representations of the same categorical values due to human entry errors, abbreviations, typos, \n",
        "# or formatting differences. Left unaddressed, these inconsistencies can fragment categories, distort aggregations, and lead to misleading \n",
        "# analytical conclusions.\n",
        "\n",
        "# In this step, I implemented custom normalization functions for key categorical fields—including technician names, cities, regions, job types, time fields,\n",
        "#  and binary outcome indicators. Each function follows a consistent strategy:\n",
        "# - Remove punctuation and unnecessary characters\n",
        "# - Trim whitespace and standardize casing\n",
        "# - Map known variations and misspellings to a single, canonical value\n",
        "\n",
        "# For fields such as technician names and locations, standardization ensures accurate grouping, performance evaluation, and reporting. For time-based fields,\n",
        "# normalizing multiple formats into a consistent representation enables reliable comparisons and downstream time-based analysis. Binary fields (e.g., \n",
        "# success and repeat visits) are normalized to enforce consistent logical values.\n",
        "\n",
        "# This rule-based approach was chosen over automated fuzzy matching to retain full control over how values are consolidated, minimize unintended merges,\n",
        "# and ensure business-relevant distinctions are preserved. The result is a cleaner, more reliable dataset that supports accurate analysis and decision-making.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "id": "BKazGX5c9hzr"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# 3. Fix inconsistent values in each columns\n",
        "# ---------------------------------------------------------\n",
        "# Normalizing tech names\n",
        "def normalize_tech(name):\n",
        "    if pd.isna(name):\n",
        "        return np.nan\n",
        "    name = re.sub(r\"[^\\w\\s]\", \"\", name)  # remove punctuation\n",
        "    name = name.strip().title()\n",
        "\n",
        "    # Map variations to standardized names\n",
        "    mapping = {\n",
        "        'John S.':'John Smith',\n",
        "        'J Smith':'John Smith',\n",
        "        'John S':'John Smith',\n",
        "        'J. Smith':'John Smith',\n",
        "        'Jess T.':'Jessica Taylor',\n",
        "        'J. Taylor':'Jessica Taylor',\n",
        "        'J Taylor':'Jessica Taylor',\n",
        "        'Jess T':'Jessica Taylor',\n",
        "        'Jessica T':'Jessica Taylor',\n",
        "        'Maria Gomez':'Maria Gomez',\n",
        "        'Maria G':'Maria Gomez',\n",
        "        'Mari Gomez':'Maria Gomez',\n",
        "        'M Gomez':'Maria Gomez',\n",
        "        'M. Gomez':'Maria Gomez',\n",
        "        'Liam P.':'Liam Peterson',\n",
        "        'Liam P':'Liam Peterson',\n",
        "        'L P':'Liam Peterson'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in name.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return name\n",
        "\n",
        "df[\"tech_name\"] = df[\"tech_name\"].apply(normalize_tech)\n",
        "#df\n",
        "\n",
        "# Normalizing city names\n",
        "def normalize_city(city):\n",
        "    if pd.isna(city):\n",
        "        return np.nan\n",
        "    city = re.sub(r\"[^\\w\\s]\", \"\", city)  # remove punctuation\n",
        "    city = city.strip().title()\n",
        "\n",
        "    # Map variations to city names\n",
        "    mapping = {\n",
        "        'concord':'Concord',\n",
        "        'charlotte':'Charlotte',\n",
        "        'huntersvill':'Huntersville'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in city.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return city\n",
        "\n",
        "df[\"city\"] = df[\"city\"].apply(normalize_city)\n",
        "#df\n",
        "\n",
        "# Normalizing region names\n",
        "def normalize_region(region):\n",
        "    if pd.isna(region):\n",
        "        return np.nan\n",
        "    region = re.sub(r\"[^\\w\\s]\", \"\", region)  # remove punctuation\n",
        "    region = region.strip().title()\n",
        "\n",
        "    # Map variations to region names\n",
        "    mapping = {\n",
        "        'east':'east',\n",
        "        'East':'east',\n",
        "        'Suth':'south',\n",
        "        'South':'south',\n",
        "        'WEST':'west',\n",
        "        'West':'west',\n",
        "        'Noth':'north',\n",
        "        'North':'north'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in region.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return city\n",
        "\n",
        "df[\"region\"] = df[\"region\"].apply(normalize_region)\n",
        "#df\n",
        "\n",
        "# Normalizing job_type names\n",
        "def normalize_job(job_type):\n",
        "    if pd.isna(job_type):\n",
        "        return np.nan\n",
        "    job_type = re.sub(r\"[^\\w\\s]\", \"\", job_type)  # remove punctuation\n",
        "    job_type = job_type.strip().title()\n",
        "\n",
        "    # Map variations for job_type\n",
        "    mapping = {\n",
        "        'install':'install',\n",
        "        'reapir':'repair',\n",
        "        'Repair':'repair',\n",
        "        'Instal':'install',\n",
        "        'Install':'install',\n",
        "        'Maintenance':'maintenence',\n",
        "        'Maint.':'maintenence',\n",
        "        'Maint':'maintenence'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in job_type.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return job_type\n",
        "\n",
        "df[\"job_type\"] = df[\"job_type\"].apply(normalize_job)\n",
        "#df\n",
        "\n",
        "# Normalizing scheduled_time\n",
        "def normalize_time(scheduled_time):\n",
        "    if pd.isna(scheduled_time):\n",
        "        return np.nan\n",
        "    scheduled_time = re.sub(r\"[^\\w\\s]\", \"\", scheduled_time)  # remove punctuation\n",
        "    scheduled_time = scheduled_time.strip().title()\n",
        "\n",
        "    # Map variations for scheduled_time\n",
        "    mapping = {\n",
        "        '14:22':'2:22pm',\n",
        "        '12:3 pm':'12:30pm',\n",
        "        '07:50':'7:50am',\n",
        "        '09:5 AM':'9:50am',\n",
        "        '16:40':'4:40pm',\n",
        "        '8:15 AM':'8:15am',\n",
        "        '1:15 pm':'1:15pm',\n",
        "        '900Am':'9:00am',\n",
        "        '1422':'2:22pm',\n",
        "        '123 pm':'12:30pm',\n",
        "        '750Am':'7:50am',\n",
        "        '0750':'7:50am',\n",
        "        '095 AM':'9:50am',\n",
        "        '1640':'4:40pm',\n",
        "        '815 AM':'8:15am',\n",
        "        '1115 pm':'1:15pm',\n",
        "        '900Am':'9:00am',\n",
        "        '115 Pm':'1:15pm',\n",
        "        '1230':'12:30pm',\n",
        "        '222Pm':'2:22pm',\n",
        "        '0950Am':'9:50am',\n",
        "        '440Pm':'4:40pm'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in scheduled_time.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return scheduled_time\n",
        "\n",
        "df[\"scheduled_time\"] = df[\"scheduled_time\"].apply(normalize_time)\n",
        "#df\n",
        "\n",
        "# Normalizing arrival_time\n",
        "def normalize_time(arrival_time):\n",
        "    if pd.isna(arrival_time):\n",
        "        return np.nan\n",
        "    arrival_time = re.sub(r\"[^\\w\\s]\", \"\", arrival_time)  # remove punctuation\n",
        "    arrival_time = arrival_time.strip().title()\n",
        "\n",
        "    # Map variations to arrival_time\n",
        "    mapping = {\n",
        "        '14:22':'2:22pm',\n",
        "        '12:3 pm':'12:30pm',\n",
        "        '07:50':'7:50am',\n",
        "        '09:5 AM':'9:50am',\n",
        "        '16:40':'4:40pm',\n",
        "        '8:15 AM':'8:15am',\n",
        "        '1:15 pm':'1:15pm',\n",
        "        '900Am':'9:00am',\n",
        "        '1422':'2:22pm',\n",
        "        '123 pm':'12:30pm',\n",
        "        '0750':'7:50am',\n",
        "        '095 AM':'9:50am',\n",
        "        '1640':'4:40pm',\n",
        "        '815 AM':'8:15am',\n",
        "        '1115 pm':'1:15pm',\n",
        "        '900Am':'9:00am',\n",
        "        '115 Pm':'1:15pm',\n",
        "        '1230':'12:30pm',\n",
        "        '222Pm':'2:22pm',\n",
        "        '0950Am':'9:50am',\n",
        "        '440Pm':'4:40pm',\n",
        "        '950Am':'9:50am',\n",
        "        '750Am':'7:50am'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in arrival_time.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return arrival_time\n",
        "\n",
        "df[\"arrival_time\"] = df[\"arrival_time\"].apply(normalize_time)\n",
        "#df\n",
        "\n",
        "# Normalizing success column\n",
        "def normalize_success(success):\n",
        "    if pd.isna(success):\n",
        "        return np.nan\n",
        "    success = re.sub(r\"[^\\w\\s]\", \"\", success)  # remove punctuation\n",
        "    success = success.strip().title()\n",
        "\n",
        "    # Map variations to arrival_time\n",
        "    mapping = {\n",
        "        'No':'No',\n",
        "        'no':'No',\n",
        "        'Yes':'Yes',\n",
        "        'yes':'Yes',\n",
        "        'Y':'Yes',\n",
        "        'N':'No'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in success.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return success\n",
        "\n",
        "df[\"success\"] = df[\"success\"].apply(normalize_success)\n",
        "\n",
        "# Normalizing repeat_visit column\n",
        "def normalize_repeat_visit(repeat_visit):\n",
        "    if pd.isna(repeat_visit):\n",
        "        return np.nan\n",
        "    repeat_visit = re.sub(r\"[^\\w\\s]\", \"\", repeat_visit)  # remove punctuation\n",
        "    repeat_visit = repeat_visit.strip().title()\n",
        "\n",
        "    # Map variations to repeat_visit\n",
        "    mapping = {\n",
        "        'No':'No',\n",
        "        'no':'No',\n",
        "        'Yes':'Yes',\n",
        "        'yes':'Yes',\n",
        "        'Y':'Yes',\n",
        "        'N':'No'\n",
        "\n",
        "    }\n",
        "\n",
        "    for key in mapping:\n",
        "        if key.lower().replace(\" \", \"\") in repeat_visit.lower().replace(\" \", \"\"):\n",
        "            return mapping[key]\n",
        "\n",
        "    return repeat_visit\n",
        "\n",
        "df[\"repeat_visit\"] = df[\"repeat_visit\"].apply(normalize_repeat_visit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Date fields frequently contain formatting inconsistencies or invalid values that can break time-based analysis if left untreated. Rather than\n",
        "# assuming all dates are correctly formatted, this step explicitly validates and corrects date entries to improve data reliability.\n",
        "\n",
        "# I first attempt to parse each value using pandas’ built-in datetime conversion with strict error handling. If a date fails validation, I then\n",
        "# apply a targeted rule to detect and correct common data-entry errors—such as swapped month and day values (e.g., an invalid month greater than 12). \n",
        "# Only when a value cannot be confidently corrected is it coerced to a missing value.\n",
        "\n",
        "# This layered approach balances data preservation with accuracy. By correcting recoverable errors while safely handling truly invalid dates, \n",
        "# I reduce unnecessary data loss and ensure that downstream analyses—such as trend analysis, scheduling performance, or seasonality—are based on valid\n",
        "# and trustworthy date information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "yRlTdhZ0_yhZ"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# 4. Fix date formats (including invalid ones)\n",
        "# ---------------------------------------------------------\n",
        "def parse_date(val):\n",
        "    if pd.isna(val):\n",
        "        return np.nan\n",
        "\n",
        "    try:\n",
        "        return pd.to_datetime(val, errors=\"raise\", dayfirst=False)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Attempt to fix invalid date like \"2024-13-10\"\n",
        "    match = re.match(r\"(\\d{4})-(\\d{2})-(\\d{2})\", str(val))\n",
        "    if match:\n",
        "        y, m, d = match.groups()\n",
        "        if int(m) > 12 and int(d) <= 12:\n",
        "            return pd.to_datetime(f\"{y}-{d}-{m}\", errors=\"coerce\")\n",
        "\n",
        "    return pd.to_datetime(val, errors=\"coerce\")\n",
        "\n",
        "df[\"scheduled_date\"] = df[\"scheduled_date\"].apply(parse_date)\n",
        "#df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-related fields often appear in multiple formats and cannot be reliably compared or analyzed as raw strings. To enable accurate time-based \n",
        "# calculations—such as arrival delays, scheduling efficiency, or duration analysis—these fields must be converted into a standardized datetime format.\n",
        "\n",
        "# In this step, I convert the scheduled and arrival time columns into pandas datetime objects using controlled coercion. Invalid or unparseable values \n",
        "# are safely converted to missing values rather than forcing incorrect interpretations. This decision prioritizes analytical integrity by ensuring that \n",
        "# only valid time values are used in downstream calculations.\n",
        "\n",
        "# Standardizing time fields at this stage allows for consistent comparisons, arithmetic operations, and feature engineering while preventing silent \n",
        "# errors that could skew performance metrics or operational insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1DTZ_QnJJ1E",
        "outputId": "8904d45c-8cd2-4b21-e1e4-7370a9801407"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3126214838.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[\"scheduled_time_dt\"] = pd.to_datetime(df[\"scheduled_time\"], errors=\"coerce\")\n",
            "/tmp/ipython-input-3126214838.py:10: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[\"arrival_time_dt\"] = pd.to_datetime(df[\"arrival_time\"], errors=\"coerce\")\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------\n",
        "# 5. Parse and standardize time columns\n",
        "# ---------------------------------------------------------\n",
        "def parse_time(t):\n",
        "    if pd.isna(t):\n",
        "        return np.nan\n",
        "    return pd.to_datetime(t, errors=\"coerce\").time()\n",
        "\n",
        "df[\"scheduled_time_dt\"] = pd.to_datetime(df[\"scheduled_time\"], errors=\"coerce\")\n",
        "df[\"arrival_time_dt\"] = pd.to_datetime(df[\"arrival_time\"], errors=\"coerce\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Even after initial normalization, minor formatting inconsistencies—such as extra internal spaces or inconsistent capitalization—can persist in\n",
        "# location fields. These small issues can still lead to duplicate categories and inaccurate grouping in analysis and reporting.\n",
        "\n",
        "# In this step, I perform a final standardization pass on the city column by collapsing extra spaces and applying consistent title casing. \n",
        "# This ensures that city names are uniformly formatted, reducing the risk of fragmented categories and improving the accuracy of location-based \n",
        "# aggregations and visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "qc_0w5uJa_jM"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# 6. Standardize city names\n",
        "# ---------------------------------------------------------\n",
        "df[\"city\"] = df[\"city\"].str.replace(\"  \", \" \").str.title()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The parts_used field contains semi-structured text with inconsistent delimiters, quotation marks, and missing-value indicators. Analyzing this field\n",
        "#  in its raw form would make it difficult to accurately assess part usage frequency, inventory needs, or repair patterns.\n",
        "\n",
        "# In this step, I transform the raw text into a structured list format by removing extraneous characters, splitting values on common delimiters, \n",
        "# and standardizing part names through consistent casing. Explicit null indicators are filtered out to avoid treating missing values as valid parts.\n",
        "\n",
        "# Converting this column into a clean, list-based representation enables more reliable downstream analysis, such as exploding part usage, counting \n",
        "# frequencies, and identifying common repair components. This approach prioritizes analytical flexibility while preserving all meaningful information from \n",
        "# the original field.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "WG9_HJJz0weQ"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# 7. Clean and standardize parts_used\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def clean_parts(p):\n",
        "    if pd.isna(p):\n",
        "        return []\n",
        "    p = p.replace('\"', \"\").replace(\"'\", \"\")\n",
        "    parts = re.split(r\"[;,]\", p)\n",
        "    parts = [x.strip().title() for x in parts if x.strip() not in null_values]\n",
        "    return parts\n",
        "\n",
        "df[\"parts_list\"] = df[\"parts_used\"].apply(clean_parts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This final step prepares the dataset for analysis and modeling by resolving remaining inconsistencies and aligning features with their intended \n",
        "# analytical use.\n",
        "\n",
        "# Unneeded raw columns are dropped once their standardized equivalents are created, reducing redundancy and improving dataset clarity. Binary outcome fields\n",
        "# are explicitly converted from string labels to boolean values to support logical operations and modeling workflows. Numeric fields are cast to appropriate\n",
        "# data types, and missing duration values are filled with zero to reflect the absence of recorded work time rather than an unknown measurement.\n",
        "\n",
        "# Column names are updated to maintain consistency after transformations, ensuring the dataset remains intuitive and easy to interpret. Finally, a targeted\n",
        "# correction is applied to replace placeholder or invalid technician names with a known value, preventing orphaned records and preserving analytical continuity.\n",
        "\n",
        "# Together, these decisions finalize a clean, well-structured dataset that is ready for reliable analysis, reporting, and downstream decision-making.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynMmF0metzuW",
        "outputId": "88614633-02a9-4c02-cd43-fae0e15d2a96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1886929491.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[cols] = df[cols].replace({\"Yes\": True, \"No\": False})\n"
          ]
        }
      ],
      "source": [
        "#---------------------------------------------------\n",
        "# 8. Last fixes\n",
        "#---------------------------------------------------\n",
        "\n",
        "# Dropping unneeded columns\n",
        "df = df.drop(columns=[\"scheduled_time\", \"arrival_time\",\"parts_used\"])\n",
        "\n",
        "# Map Yes → True, No → False\n",
        "cols = [\"success\", \"repeat_visit\"]\n",
        "df[cols] = df[cols].replace({\"Yes\": True, \"No\": False})\n",
        "\n",
        "# Changing column to numeric\n",
        "df['duration_minutes'] = df['duration_minutes'].astype(float)\n",
        "\n",
        "# Fill null values with 0 in duration_minutes column\n",
        "df[\"duration_minutes\"] = df[\"duration_minutes\"].fillna(0)\n",
        "\n",
        "# Rename column\n",
        "df.rename(columns={\"parts_list\": \"parts_used\"}, inplace=True)\n",
        "\n",
        "# Adding technician name where name was show 'Nan'\n",
        "df.loc[df['tech_name'] == 'Nan', 'tech_name'] = 'Sophia Martinez'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The original dataset did not reflect real-world outcomes. To make the project more realistic, success probabilities are assigned by job type using\n",
        "# defined target rates. This probabilistic update ensures the `success` column better represents expected operational performance while preserving variability\n",
        "#  for analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "Xpd7Zp-5RhuQ"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------------------\n",
        "# These next few blocks of code is help from ChatGpt to make my dataset more realistic\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define target success rates\n",
        "success_rates = {\n",
        "    'install': 0.80,\n",
        "    'maintenance': 0.90,\n",
        "    'repair': 0.85\n",
        "}\n",
        "\n",
        "# Update success column probabilistically by job_type\n",
        "for job_type, rate in success_rates.items():\n",
        "    mask = df['job_type'] == job_type\n",
        "    df.loc[mask, 'success'] = np.random.rand(mask.sum()) < rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To reflect realistic operational behavior, repeat visits are probabilistically assigned based on whether the service was successful. If no repeat is \n",
        "# needed, the `repeat_reason` field is cleared to maintain data consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "fMsCkp3QeXj6"
      },
      "outputs": [],
      "source": [
        "# Set repeat_visit based on success\n",
        "df['repeat_visit'] = False\n",
        "df.loc[df['success'] == False, 'repeat_visit'] = np.random.rand((df['success'] == False).sum()) < 0.7  # 60–80%\n",
        "df.loc[df['success'] == True, 'repeat_visit'] = np.random.rand((df['success'] == True).sum()) < 0.10  # 5–15%\n",
        "\n",
        "# Clear repeat_reason if no repeat visit\n",
        "df.loc[df['repeat_visit'] == False, 'repeat_reason'] = np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Service durations are updated with realistic ranges based on job type to better reflect actual operational times. This ensures the `duration_minutes` column\n",
        "#  provides plausible values for analysis and modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "bZBXUC5Reb5I"
      },
      "outputs": [],
      "source": [
        "# Define realistic duration ranges by job_type\n",
        "duration_ranges = {\n",
        "    'install': (45, 180),\n",
        "    'maintenance': (30, 90),\n",
        "    'repair': (20, 120)\n",
        "}\n",
        "\n",
        "for job_type, (min_dur, max_dur) in duration_ranges.items():\n",
        "    mask = df['job_type'] == job_type\n",
        "    df.loc[mask, 'duration_minutes'] = np.random.randint(min_dur, max_dur+1, mask.sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To make the dataset more realistic, success and repeat visits are adjusted by technician skill. Each technician is assigned a performance profile, \n",
        "# reflecting stronger or weaker success rates and likelihood of repeat visits after failure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "vn4p9CKRef_c"
      },
      "outputs": [],
      "source": [
        "techs = ['Jessica Taylor', 'John Smith', 'Nan', 'Liam Peterson', 'Maria Gomez']\n",
        "df['tech_name'] = np.random.choice(techs, size=len(df))\n",
        "\n",
        "# Assign performance tiers\n",
        "performance = {\n",
        "    'Jessica Taylor': {'success': 0.95, 'repeat_prob_fail': 0.3},  # Strong tech\n",
        "    'John Smith': {'success': 0.60, 'repeat_prob_fail': 0.9},  # Weak tech\n",
        "    'Nan': {'success': 0.85, 'repeat_prob_fail': 0.5},  # Average\n",
        "    'Liam Peterson': {'success': 0.85, 'repeat_prob_fail': 0.5},\n",
        "    'Maria Gomez': {'success': 0.85, 'repeat_prob_fail': 0.5}\n",
        "}\n",
        "\n",
        "# Update success and repeat_visit based on tech\n",
        "for tech, params in performance.items():\n",
        "    mask = df['tech_name'] == tech\n",
        "    df.loc[mask, 'success'] = np.random.rand(mask.sum()) < params['success']\n",
        "    fail_mask = mask & (df['success'] == False)\n",
        "    df.loc[fail_mask, 'repeat_visit'] = np.random.rand(fail_mask.sum()) < params['repeat_prob_fail']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To simulate real-world conditions, arrival times are adjusted with normally distributed noise around the scheduled time. This creates realistic \n",
        "# deviations while keeping extreme values within ±60 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "2urFvt-BfJZF"
      },
      "outputs": [],
      "source": [
        "# Ensure timestamps are datetime\n",
        "df['scheduled_time_dt'] = pd.to_datetime(df['scheduled_time_dt'])\n",
        "df['arrival_time_dt'] = pd.to_datetime(df['arrival_time_dt'])\n",
        "\n",
        "# Add normal noise (mean +7 min, std 15 min), cap ±60 min\n",
        "noise = np.random.normal(loc=7, scale=15, size=len(df))\n",
        "noise = np.clip(noise, -60, 60)\n",
        "df['arrival_time_dt'] = df['scheduled_time_dt'] + pd.to_timedelta(noise, unit='m')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To reflect realistic customer behavior, existing customer IDs are randomly assigned to ~85% of rows, leaving some entries as NaN to represent first-time \n",
        "# customers. This balances repeat and new customer scenarios for analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "KtGTtVH8fZPn"
      },
      "outputs": [],
      "source": [
        "# Assume existing customer IDs: df['customer_id']\n",
        "existing_customers = df['customer_id'].dropna().unique()\n",
        "\n",
        "# Assign customers to ~85% of rows\n",
        "mask = np.random.rand(len(df)) < 0.85\n",
        "df.loc[mask, 'customer_id'] = np.random.choice(existing_customers, size=mask.sum())\n",
        "\n",
        "# Optional: leave some NaNs for first-time customers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The fully cleaned and realistically adjusted dataset is saved to a CSV file for analysis or modeling. This ensures all transformations are preserved\n",
        "#  and the data is ready for downstream use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1m4y76t3rKu",
        "outputId": "f205ba7a-9b7b-4a38-daf1-cbbb2a1048a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning complete! Saved as tech_performance_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------\n",
        "# Save cleaned output\n",
        "# ---------------------------------------------------------\n",
        "df.to_csv(\"tech_performance_cleaned.csv\", index=False)\n",
        "\n",
        "print(\"Cleaning complete! Saved as tech_performance_cleaned.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVRF8AiChoGJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
